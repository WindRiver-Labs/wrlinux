#!/usr/bin/env python3
#
# Copyright (C) 2020 Wind River Systems, Inc.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License version 2 as
# published by the Free Software Foundation.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
# See the GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA

import os
import sys
import subprocess
import argparse
import logging
import shutil
import glob
import time
import hashlib
import git
import socket

logger = logging.getLogger('gen-image')

def get_project():
    runqemu_dir = os.path.realpath(shutil.which("runqemu"))
    return os.path.realpath("%s/../../../../" % runqemu_dir)

def run_cmd(cmd, logger=logger):
    logger.info('Running %s' % cmd)

    try:
        output = subprocess.check_output(cmd, shell=True, stderr=subprocess.STDOUT)
        output = output.decode('utf-8')
        logger.debug('output: %s' % output)
    except subprocess.CalledProcessError as e:
        raise Exception("%s\n%s" % (str(e), e.output.decode('utf-8')))

    return output

def run_bitbake_cmd(target, logger):
    cmd = 'bitbake %s' % target
    output = run_cmd(cmd)
    return output

def set_logger(logger):
    logger.setLevel(logging.DEBUG)

    class ColorFormatter(logging.Formatter):
        FORMAT = ("$BOLD%(name)-s$RESET - %(levelname)s: %(message)s")

        BLACK, RED, GREEN, YELLOW, BLUE, MAGENTA, CYAN, WHITE = list(range(8))

        RESET_SEQ = "\033[0m"
        COLOR_SEQ = "\033[1;%dm"
        BOLD_SEQ = "\033[1m"

        COLORS = {
            'WARNING': YELLOW,
            'INFO': GREEN,
            'DEBUG': BLUE,
            'ERROR': RED
        }

        def formatter_msg(self, msg, use_color = True):
            if use_color:
                msg = msg.replace("$RESET", self.RESET_SEQ).replace("$BOLD", self.BOLD_SEQ)
            else:
                msg = msg.replace("$RESET", "").replace("$BOLD", "")
            return msg

        def __init__(self, use_color=True):
            msg = self.formatter_msg(self.FORMAT, use_color)
            logging.Formatter.__init__(self, msg)
            self.use_color = use_color

        def format(self, record):
            levelname = record.levelname
            if self.use_color and levelname in self.COLORS:
                fore_color = 30 + self.COLORS[levelname]
                levelname_color = self.COLOR_SEQ % fore_color + levelname + self.RESET_SEQ
                record.levelname = levelname_color
            return logging.Formatter.format(self, record)

    # create console handler and set level to debug
    ch = logging.StreamHandler()
    ch.setLevel(logging.DEBUG)
    ch.setFormatter(ColorFormatter())
    logger.addHandler(ch)

def get_today():
    return time.strftime("%Y%m%d%H%M%S")

def get_sha256(f):
    with open(f, 'rb') as inf:
        data = inf.read()
    m = hashlib.sha256()
    m.update(data)
    return m.hexdigest()

set_logger(logger)

def check_and_write(outfile, line):
    with open(outfile, 'a+') as f:
        f.seek(0)
        if not line in f.readlines():
            logger.info('Writting "%s" to %s' % (line.strip(), outfile))
            f.write(line)

class GenImage(object):
    """
    * Generate the following images in order:
        - Linux SDK
        - target images
        - container image
    * world build for packages feeds
    * Save/Re-use PR database prserv.sqlite3
    * Gen sources for IP review
    """

    def __init__(self):
        self.my_dir = os.path.abspath(os.path.dirname(sys.argv[0]))
        self.data_dir = os.path.join(self.my_dir, 'data')
        self.bin_image_conf = os.path.join(self.data_dir, 'wrlinux-bin-image.conf')
        self.container_conf = os.path.join(self.data_dir, 'wrlinux-container.conf')
        self.ip_conf = os.path.join(self.data_dir, 'wrlinux-ip.conf')

        parser = argparse.ArgumentParser(
            description="Gen images for specified machine",
            epilog="Use %(prog)s --help to get help")
        parser.add_argument('-m', '--machine',
            default='',
            help='Specify machine')
        parser.add_argument('-r', '--ip-review',
            default=False,
            help='Only generate sources for IP review', action="store_true")
        parser.add_argument('-i', '--build-images',
            default=False,
            help='Generate binary images and repos, this is the default action. -r -i will generate both images and sources', action="store_true")
        parser.add_argument('-b', '--pr-database',
            default=os.path.join(self.data_dir, 'prserv.sqlite3'),
            help='Specify PR database to use, no database will be used if specified no', action="store")
        parser.add_argument('-o', '--outdir',
            default=None,
            help='Specify the output dir', action="store")
        parser.add_argument('-u', '--upload-url',
            default=None,
            help='Specify the url to upload the files, which uses rsync to upload the files', action="store")
        parser.add_argument('-c', '--config',
            default=None,
            help="Read configuration from file, such as DISTRO, MACHINE and layers' branches", action="store")
        parser.add_argument('--rpm-url',
            default=None,
            help='Specify the url to download (rsync) rpm packages, these packages will be used by packagefeed-stability.bbclass, the RPM_URL is the prepfix of the url, e.g, rsync://RPM_URL/<machine>/repos/rpm/', action="store")
        parser.add_argument('-p', '--product', choices=['lincd', 'lts-21'], required=False,
            default='lts-21',
            help='Specify the product', action="store")
        parser.add_argument("-d", "--debug",
            help = "Enable debug output",
            action="store_const", const=logging.DEBUG, dest="loglevel", default=logging.INFO)
        parser.add_argument("-q", "--quiet",
            help = "Hide all output except error messages",
            action="store_const", const=logging.ERROR, dest="loglevel")

        self.args = parser.parse_args()

        logger.setLevel(self.args.loglevel)

        bitbake_path = shutil.which('bitbake')
        self.builddir = os.getenv('BUILDDIR')
        if not (bitbake_path and self.builddir):
            raise Exception('bitbake command is not found, this tools should be run after source oe-init-build-env')

        self.today = get_today()

        self.project = get_project()

        self.machine = self.args.machine

        self.image_minimal = 'wrlinux-image-minimal'
        self.image_full = 'wrlinux-image-full'
        self.image = os.getenv('IMAGE', '')
        if self.image:
            logger.info('Got IMAGE %s from environment vars' % self.image)
            self.images = [self.image]
        else:
            # Default image
            self.image = self.image_full
            self.images = [self.image_minimal, self.image_full]

        self.doc_dir = os.path.join(os.path.dirname(sys.argv[0]), 'doc')

        self.local_layer = os.path.join(self.project, 'layers/local')

        self.local_conf = os.path.join(self.builddir, 'conf/local.conf')
        self.bblayers_conf = os.path.join(self.builddir, 'conf/bblayers.conf')

        if self.args.product == 'lincd':
            self.handoff_name = 'WRLinux-CD-Images'
        elif self.args.product == 'lts-21':
            self.handoff_name = 'WRLinux-lts-21-Images'

        if self.args.outdir:
            self.outdir = self.args.outdir
        else:
            self.outdir = os.path.join(self.builddir, 'outdir')

        self.handoff_dir = os.path.join(self.outdir, self.handoff_name)

        self.cache_dir = os.path.join(self.builddir, 'cache')
        if not os.path.exists(self.cache_dir):
            os.makedirs(self.cache_dir)

        self.pr_db = os.path.join(self.cache_dir, 'prserv.sqlite3')
        self.pr_log = os.path.join(self.builddir, 'cache/prserv.log')
        self.pr_dir = os.path.join(self.handoff_dir, 'prserv')

        self.deploy_dir_rpm = ""

    def read_config(self):
        logger.info('Reading configuration from %s' % self.args.config)
        distro = ""
        self.layers = {}
        with open (self.args.config, 'r') as f:
            for line in f.readlines():
                if not "=" in line:
                    continue
                line_split = line.split()
                if len(line_split) < 3 or not line_split:
                    continue
                if line_split[0] == 'DISTRO':
                    distro = line_split[2]
                elif line_split[0] == 'MACHINE':
                    self.machine = line_split[2].strip('"')
                elif line_split[0] == 'WRLINUX_VERSION':
                    wrlinux_version = line_split[2].strip('"')
                elif line_split[0] == 'WRLINUX_BRANCH':
                    wrlinux_branch = line_split[2].strip('"')
                elif ":" in line_split[2]:
                    layername = line_split[0]
                    if layername == 'local':
                        continue
                    branch_id = line_split[2]
                    id = branch_id.split(':')[1].strip('"')
                    self.layers[layername] = id

            if distro:
                with open(self.local_conf, 'a') as f:
                    f.write('DISTRO = %s\n' % distro)

        # Figure out RCPL versions
        version = '_'.join(wrlinux_version.split('.')[:2])
        rcpl = int(wrlinux_version.split('.')[-1])
        if rcpl < 10:
            rcpl = "000%s" % rcpl
        elif rcpl < 100:
            rcpl = "00%s" % rcpl
        elif rcpl < 1000:
            rcpl = "0%s" % rcpl

        if wrlinux_branch == "LTS":
            version = "WRLINUX_%s_LTS_RCPL%s" % (version, rcpl)
        elif wrlinux_branch == "Base":
            version = "WRLINUX_%s_BASE_UPDATE%s" % (version, rcpl)
        else:
            logger.warning('Failed to figure out rcpl version')
            version = ""
        logger.debug('Version: %s' % version)

        # Find all layers
        cmd = "find %s/layers -path  '*/conf/layer.conf' -type f" % self.project
        layer_confs = run_cmd(cmd).split('\n')

        def checkout_branch(candidates, commit_id):
            checkedout = False
            for c in candidates:
                logger.debug('Checking out %s to %s' % (c, commit_id))
                g = git.cmd.Git(c)
                try:
                    g.checkout(commit_id)
                    checkedout = True
                    break
                except Exception as esc:
                    logger.debug(esc)

            return checkedout

        logger.info('Checking out layers according to %s' % self.args.config)
        handled_ids = []
        for layername in self.layers:
            candidates = []
            for conf in layer_confs:
                layer_conf = '/%s/conf/layer.conf' % layername
                # It may match mutiple layers, so use candidates to save them
                if conf.endswith(layer_conf):
                    layer = os.path.dirname(os.path.dirname(conf))
                    candidates.append(layer)

            commit_id = self.layers[layername]
            if commit_id in handled_ids:
                continue
            handled_ids.append(commit_id)
            checkout_branch(candidates, commit_id)
            if not checkout_branch(candidates, commit_id):
                logger.warning('Failed to checkout %s to %s, try --debug for more info' % (layername, commit_id))
                if version:
                    logger.warning('Trying to checkout %s to %s as a workaround' % (layername, version))
                    if not checkout_branch(candidates, version):
                        logger.warning('Failed to checkout %s to %s, try --debug for more info' % (layername, version))
                    else:
                        logger.warning('Checked-out %s to %s, but this may not work' % (layername, version))

        # The gen-image dir must be up-to-date
        g = git.cmd.Git(self.my_dir)
        g.checkout('m/master', self.my_dir)

    def prepare(self):

        line = 'require %s\n' % self.bin_image_conf
        check_and_write(self.local_conf, line)

        line = 'BBLAYERS += "%s/layers/wrlinux/wrlinux-kernel-dev"\n' % self.project
        check_and_write(self.bblayers_conf, line)

        bitbake_e = ''
        logger.info('Figuring out machine ...')
        bitbake_e = self.do_build('-e')
        for line in bitbake_e.split('\n'):
            if line.startswith('PKGDATA_DIR='):
                self.machine = line.split('=')[1].replace('"', '')
                self.machine = os.path.basename(self.machine)
                break

        # Only write MACHINE when self.args.machine != MACHINE from bitbake_e
        if self.args.machine and self.machine != self.args.machine:
            self.machine = self.args.machine
            with open(self.local_conf, 'a+') as f:
                f.write('\nMACHINE = "%s"\n' % self.args.machine)
                # Need need re-run 'bitbake -e'
                bitbake_e = ''

        logger.info('Preparing build for %s' % self.machine)
        self.repos_dir = os.path.join(self.handoff_dir, self.machine, 'repos')

        self.machines_ws = ["intel-x86-64", "bcm-2xxx-rpi4", "nxp-imx8"]
        self.handoff_name_ws = "%s-%s" % (self.handoff_name, self.machine)
        self.handoff_dir_ws = os.path.join(self.handoff_dir, self.handoff_name_ws)

        if self.args.pr_database and os.path.exists(self.args.pr_database):
            if not self.args.pr_database.endswith('.sqlite3'):
                raise Exception('Invalid PR databse file: %s' % self.args.pr_database)
            # Compare pr_db's sizes, use the larger one.
            from_size = os.path.getsize(self.args.pr_database)
            to_size = 0
            if os.path.exists(self.pr_db):
                to_size = os.path.getsize(self.pr_db)
            if from_size > to_size:
                logger.info('Using %s as pr database' % self.args.pr_database)
                shutil.copy(self.args.pr_database, self.pr_db)
            else:
                logger.info('Using %s as pr database' % self.pr_db)
        elif self.args.pr_database in ('no', 'No', 'NO') and os.path.exists(self.pr_db):
            logger.info('Removing %s' % self.pr_db)
            os.remove(self.pr_db)

        if self.args.build_images or (self.machine and not self.args.ip_review):
            self.mv_to_old(self.machine)
            self.mv_to_old(self.handoff_name_ws)

        if not os.path.exists(self.handoff_dir):
            os.makedirs(self.handoff_dir)

        self.feed_archs = []
        all_archs = []
        if not bitbake_e:
            bitbake_e = self.do_build('-e')

        feed_uris_defined = False
        self.support_lat = False
        for line in bitbake_e.split('\n'):
            if line.startswith('PACKAGE_FEED_URIS='):
                feed_uris_defined = True
            if line.startswith('PACKAGE_FEED_ARCHS='):
                self.feed_archs = line.split('=')[1].replace('"', '').split()
            if line.startswith('ALL_MULTILIB_PACKAGE_ARCHS='):
                all_archs = line.split('=')[1].replace('"', '').split()
            if line.startswith('TMPDIR='):
                self.tmpdir = line.split('=')[1].replace('"', '')
            if line.startswith('WRTEMPLATE='):
                self.templates = line.split('=')[1].replace('"', '')
            if line.startswith('DEPLOY_DIR_RPM='):
                self.deploy_dir_rpm = line.split('=')[1].replace('"', '')

        if 'feature/lat' in self.templates:
            self.support_lat = True

        if not self.feed_archs:
            if not all_archs:
                raise Exception("Failed to figure out feed_archs")
            logger.info("PACKAGE_FEED_ARCHS is not defined, set it automatically")
            for arch in all_archs:
                self.feed_archs.append(arch.replace("-", "_"))

        if not self.tmpdir:
            raise Exception("Failed to figure tmpdir!")

        if not feed_uris_defined:
            port = '9310'
            hostname = socket.gethostname()
            uri = 'http://%s:%s/outdir' % (hostname, port)
            logger.warning('PACKAGE_FEED_URIS is not defined, setting it to %s' % uri)

            line = 'PACKAGE_FEED_URIS = "%s"\n' % uri
            check_and_write(self.local_conf, line)

        if self.args.rpm_url:
            self.download_rpm()

    def mv_to_old(self, src_name):
        src = os.path.join(self.handoff_dir, src_name)
        old_dir = os.path.join(self.outdir, 'old')
        dst = os.path.join(old_dir, '%s.%s' % (os.path.basename(src), self.today))
        if not os.path.exists(old_dir):
            os.makedirs(old_dir)
        if os.path.exists(src):
            shutil.move(src, dst)

    def do_build(self, target=None, conf=None):
        if conf:
            with open(self.local_conf, 'a') as f:
                f.write(conf)

        if not target:
            target = self.image
        output = run_bitbake_cmd(target, logger)
        return output

    def do_deploy(self, subdir, files):
        logger.info('Deploying %s' % subdir)

        def do_copy(src, dst_dir):
            if not os.path.exists(dst_dir):
                os.makedirs(dst_dir)
            shutil.copy(src, dst_dir)

        # The image file copy once, ovmf and document copy twice into the
        # following dirs:
        # - container-minimal
        # - container-full
        # - target-minimal
        # - target-full
        dst_dir = os.path.join(self.handoff_dir, subdir)
        dst_dir_minimal = '%s-minimal-%s' % (dst_dir, self.machine)
        dst_dir_full = '%s-full-%s' % (dst_dir, self.machine)
        dst_dir_sdk = '%s-%s' % (dst_dir, self.machine)
        copy_twice = False
        if self.image_minimal in ' '.join(files):
            copy_twice = True
        for f in files:
            if subdir.endswith('/sdk'):
                do_copy(f, dst_dir_sdk)
            elif self.image_minimal in f:
                do_copy(f, dst_dir_minimal)
            elif self.image_full in f:
                do_copy(f, dst_dir_full)
            elif copy_twice:
                do_copy(f, dst_dir_minimal)
                do_copy(f, dst_dir_full)
            else:
                do_copy(f, dst_dir)

    def gen_sdk(self):
        logger.info('Generating sdk...')

        self.do_build('%s -cpopulate_sdk' % self.image)
        deploy_files = []

        deploy_dir = os.path.join(self.tmpdir, 'deploy/sdk')
        suffixes = ['sdk.sh', 'sdk.host.manifest', 'sdk.target.manifest']
        for f in suffixes:
            wildcard = "%s/*-%s-%s-%s" % (deploy_dir, self.machine.replace('-', '_'), self.image, f)
            files = glob.glob(wildcard)
            if len(files) != 1:
                raise Exception("Should be only one %s file but found %s: \n%s" % (f, len(files), '\n'.join(files)))
            deploy_files.append(files[0])

        deploy_files.append(os.path.join(self.doc_dir, 'sdk.README.md'))
        if self.support_lat:
            deploy_files.append(os.path.join(self.doc_dir, 'appsdk.README.md'))

        deploy_files.append(self.get_configinfo_file())
        self.do_deploy('%s/sdk' % self.machine, deploy_files)

    def get_deploy_files(self, type='', uboot=True, images=[]):
        deploy_dir = os.path.join(self.tmpdir, 'deploy/images/%s' % self.machine)
        deploy_files = []
        if not images:
            images = self.images
        for image in images:
            prefix = '%s/%s-%s' % (deploy_dir, image, self.machine)
            if type:
                deploy_files.append('%s.%s' % (prefix, type))
            deploy_files.append('%s.manifest' % prefix)
        ovmf = os.path.join(deploy_dir, 'ovmf.qcow2')
        if os.path.exists(ovmf):
            deploy_files.append(ovmf)
        if uboot:
            if self.machine == "bcm-2xxx-rpi4":
                qemu_u_boot = os.path.join(deploy_dir, 'qemu-u-boot-bcm-2xxx-rpi4.bin')
                deploy_files.append(qemu_u_boot)
        for f in deploy_files:
            logger.debug('Deploy file: %s' % f)
            if not os.path.exists(f):
                raise Exception('Failed to find %s' % f)

        return deploy_files

    def get_configinfo_file(self):
        # Save config info
        config_outdir = os.path.join(self.tmpdir, 'deploy/images/%s' % self.machine)
        if not os.path.exists(config_outdir):
            os.makedirs(config_outdir)
        config_outfile = os.path.join(config_outdir, 'config_%s' % self.machine)

        logger.info('Generating %s' % config_outfile)
        newlines = []
        logfile = 'tmp-%s-glibc/log/cooker/%s/console-latest.log' % (self.machine, self.machine)
        with open(logfile, 'r') as f:
            for line in f.readlines():
                if line.startswith('Build Configuration:'):
                    newlines.append(line)
                    continue
                if line.startswith('--------------------'):
                    break
                if newlines:
                    newlines.append(line)
        if newlines:
            with open(config_outfile, 'w') as f:
                f.write(''.join(newlines))
        else:
            logger.warning('Failed to find config info from %s' % logfile)
        return (config_outfile)

    def gen_ostree(self):
        logger.info('Generating target image...')

        deploy_files = set()
        is_x86 = 'intel' in self.machine or 'x86-64' in self.machine

        # Remove ostreerepo_dir to avoid mixing minimal and full, otherwise the
        # minimal's size would be large in the second run.
        ostreerepo_dir = os.path.join(self.tmpdir, 'deploy/images/%s/ostree_repo' % self.machine)
        if os.path.exists(ostreerepo_dir):
            ostreerepo_dir_saved = '%s.%s.%s' % (ostreerepo_dir, self.machine, self.today)
            shutil.move(ostreerepo_dir, ostreerepo_dir_saved)
        else:
            logger.debug("%s doesn't exist" % ostreerepo_dir)

        self.do_build('%s -cclean' % ' '.join(self.images))
        for image in self.images:
            self.do_build(target=image)

            # Regenerate wic image to support disk resize
            if os.path.exists(ostreerepo_dir):
                cmd = "%s/layers/wr-ostree/scripts/bootfs.sh -L -b %s -s 0 -a 'instdate=BUILD_DATE instw=60'" % (self.project, image)
                run_cmd(cmd)

            new_image = ""
            deploy_files |= set(self.get_deploy_files(uboot=not is_x86, images=[image]))
            basename = '%s-%s' % (image, self.machine)
            ustart_images = ['ustart.img.bmap']
            if self.machine == 'nxp-s32g2xx':
                ustart_images += ['evb.ustart.img.gz', 'rdb2.ustart.img.gz']
            else:
                ustart_images.append('ustart.img.gz')

            for f in ustart_images:
                if self.machine == 'nxp-s32g2xx':
                    newf = os.path.join(self.builddir, '%s-%s' % (basename, f))
                else:
                    newf = os.path.join(self.builddir, '%s.%s' % (basename, f))
                os.rename(f, newf)
                deploy_files.add(newf)

        deploy_files.add(self.get_configinfo_file())

        readme = os.path.join(self.doc_dir, 'target_%s.README.md' %  self.machine)
        if os.path.exists(readme):
            deploy_files.add(readme)
        else:
            logger.warning('No README: %s' % readme)

        write_sd_sh = os.path.join(self.doc_dir, '%s-sd.sh' %  self.machine)
        if os.path.exists(write_sd_sh):
            deploy_files.add(write_sd_sh)

        self.do_deploy('%s/target' % self.machine, deploy_files)

        logger.info('Deploying %s/repos/ostree_repo...' % self.machine)
        dst_dir = os.path.join(self.repos_dir, 'ostree_repo')
        if os.path.exists(ostreerepo_dir):
            shutil.copytree(ostreerepo_dir, dst_dir)

    def gen_container(self):
        logger.info('Generating container image...')

        for image in self.images:
            self.do_build(target='%s -r %s' % (image, self.container_conf))

        deploy_files = self.get_deploy_files(type='tar.bz2', uboot=False)

        deploy_files.append(os.path.join(self.doc_dir, 'container.README.md'))
        deploy_files.append(self.get_configinfo_file())
        self.do_deploy('%s/container' % self.machine, deploy_files)

    def gen_feeds(self):
        logger.info('Generating packages feeds...')
        # The docker won't be built in a world build since it is started by virtual.
        self.do_build('world docker %s' % ' '.join(self.images))

        # Remove unneeded archs
        rpm_dir = os.path.join(self.tmpdir, 'deploy/rpm')
        existed_archs = os.listdir(rpm_dir)
        for e in existed_archs:
            if e.startswith('.pkgdata.tar.bz2'):
                continue
            if not e in self.feed_archs:
                e_path = os.path.join(rpm_dir, e)
                logger.info('Removing unneeded data: %s' % e_path)
                shutil.rmtree(e_path)

        self.do_build('package-index')

        if not os.path.exists(self.repos_dir):
            os.makedirs(self.repos_dir)

        logger.info('Deploying %s/repos/rpm' % self.machine)
        dst_dir = os.path.join(self.repos_dir, 'rpm')
        shutil.copytree(rpm_dir, dst_dir)

    def post(self):
        with open(self.local_conf, 'a') as f:
            f.write('#### Done by gen-image\n')

        # Save local.conf
        local_conf_machine = '%s.%s' % (self.local_conf, self.machine)
        shutil.copy(self.local_conf, local_conf_machine)
        logger.info('The local.conf is saved to %s' % local_conf_machine)

        # Save prserver
        self.mv_to_old('prserv')
        os.makedirs(self.pr_dir)
        shutil.copy(self.pr_db, self.pr_dir)
        shutil.copy(self.pr_log, self.pr_dir)

        # Generate sha256 for the following files
        # - container/*.tar.bz2
        # - target/*.img.gz ovmf.qcow2
        # - sdk/*.sh
        tgts = ("container-*/*.tar.bz2", "target-*/*.img.gz", "target-*/ovmf.qcow2", "sdk/*.sh")
        sha256files = []
        file_prefix = os.path.join(self.handoff_dir, self.machine)
        for tgt in tgts:
            fmt = os.path.join(file_prefix, tgt)
            for f in glob.glob(fmt):
                sha256files.append(f)
        outlist = []
        if sha256files:
            for sha256file in sha256files:
                basename = os.path.basename(sha256file)
                logger.info('Generating checksum for %s' % basename)
                dirname = os.path.dirname(sha256file)
                outfile = os.path.join(dirname, 'sha256sum.txt')
                with open(outfile, 'a+') as f:
                    f.write('%s *%s\n' % (get_sha256(sha256file), basename))
        else:
            logger.warning("The checksum is not generated since no files are found!")

        # Create tarballs for target minimal/full and sdk directories
        if os.path.exists(file_prefix):
            for d in os.listdir(file_prefix):
                if ('-minimal-' in d or '-full-' in d or 'sdk-' in d) and not d.startswith('container-'):
                    tarname = '%s.tar.bz2' % d
                    tarname_path = os.path.join(file_prefix, tarname)
                    cmd = 'tar --numeric-owner -cjvf %s %s' % (tarname, d)
                    logger.info('Running %s' % cmd)
                    subprocess.check_output(cmd, shell=True, cwd=file_prefix)

                    if self.machine in self.machines_ws:
                        if not os.path.exists(self.handoff_dir_ws):
                            os.makedirs(self.handoff_dir_ws)
                        shutil.copy(tarname_path, self.handoff_dir_ws)

        logger.info('Deploy files: %s and WRLinux-lts-21-Images-%s' % (os.path.join(self.handoff_dir, self.machine), self.machine))

    def ip_review(self):
        logger.info('Generating sources for IP review')
        logger.info('Using prefile %s' % self.ip_conf)
        # The docker won't be built in a world build since it is started with "virtual/".
        self.do_build('world docker %s -r %s --runall=deploy_archives' % (' '.join(self.images), self.ip_conf))

        # And sdk
        # The "bitbake wrlinux-image-full --runall=deploy_archives -cpopulate_sdk"
        # doesn't work, so we figure out the list and build them one by one
        self.do_build('%s -r %s -cpopulate_sdk -g' % (self.image, self.ip_conf))
        recipes = []
        with open('pn-buildlist') as f:
            for line in f.readlines():
                line = line.strip()
                # Only native sdk recipes are needed since others have already been built
                if line.startswith('nativesdk-') and line != 'nativesdk-libgcc-initial':
                    recipes.append(line)

        self.do_build('%s -r %s  -cdeploy_archives' % (' '.join(recipes), self.ip_conf))

        src_dir = os.path.join(self.tmpdir, 'deploy/sources')
        logger.info('Deploying sources-%s' % self.machine)
        dst_dir = os.path.join(self.handoff_dir, 'sources-%s' % self.machine)
        self.mv_to_old('sources-%s' % self.machine)
        os.makedirs(dst_dir)
        # Use hardlinks
        cmd = 'cp -al %s/*/* %s' % (src_dir, dst_dir)
        run_cmd(cmd)
        # Remove natives
        cmd = 'rm -fr %s/*-native-*' % dst_dir
        run_cmd(cmd)

        # Archive the sources
        if os.path.exists(dst_dir):
            logger.info('Creating tarballs in %s...' % dst_dir)
            if shutil.which('pigz'):
                logger.info('Using pigz...')
                tar_opt = '-I pigz -cf'
            else:
                tar_opt = '-czf'
            sources_list = os.path.join(dst_dir, 'sources-list-%s.txt' % self.machine)
            sf = open(sources_list, 'w')
            for i in os.listdir(dst_dir):
                sumfiles = []
                # Generate checksums
                topdir = '%s/%s' % (dst_dir, i)
                if not os.path.isdir(topdir):
                    continue
                for root, dirs, files in os.walk(topdir):
                    for f in files:
                        f_path = '%s/%s' % (root, f)
                        sumfiles.append(f_path.replace(topdir + '/', ''))

                sumfiles.sort()
                # Save files' sha256 to outfile, then use outfile's sha256 as
                # the tarball's suffix, the outfile is mainly used for debugging.
                outfile = '%s/gen-image-sha256sum.txt' % topdir
                with open(outfile, 'w') as f:
                    for s in sumfiles:
                        sha256 = ""
                        # Use the HEAD commit's ID as sha256 for git repo
                        s_path = '%s/%s' % (topdir, s)
                        if s.endswith('.tar.gz'):
                            cmd = "tar --list --wildcards '*/*/.git/HEAD' -zf %s" % s_path
                            retval, output = subprocess.getstatusoutput(cmd)
                            if retval == 0:
                                # Only extract the "./git" dir
                                cmd = "tar -x --wildcards '*/*/.git/' -zf %s" % s_path
                                subprocess.check_call(cmd, shell=True, cwd=topdir)
                                g = git.cmd.Git('%s/%s' % (topdir, os.path.dirname(output)))
                                sha256 = g.log('-1', '--format=%H', 'HEAD') + '(git)'
                                extracted_dir = output.split('/')[0]
                                if extracted_dir:
                                    shutil.rmtree('%s/%s' % (topdir, extracted_dir))
                                else:
                                    logger.warning('Failed to figure out extracted_dir: %s' % output)
                            elif s.startswith(i):
                                # The archiver.bbclass creates a tarball for 'dir'
                                # sources with a random directory name, which makes
                                # the checksum mistmatch, so save checksum of in
                                # the dir rather than the tarball itself to fix the
                                # checksum issues.
                                cmd = "tar --list --wildcards 'tmp*/' -zf %s" % s_path
                                retval, output = subprocess.getstatusoutput(cmd)
                                if retval == 0:
                                    cmd = "tar -xzf %s" % s_path
                                    subprocess.check_call(cmd, shell=True, cwd=topdir)
                                    extracted_dir = output.split('/')[0]
                                    if extracted_dir:
                                        extracted_dir_path = '%s/%s' % (topdir, extracted_dir)
                                        for root, dirs, files in os.walk(extracted_dir_path):
                                            for file in files:
                                                f_path = '%s/%s' % (root, file)
                                                if sha256:
                                                    sha256 += "\n%s" % get_sha256(f_path)
                                                else:
                                                    sha256 = get_sha256(f_path)
                                        shutil.rmtree(extracted_dir_path)
                                    else:
                                        logger.warning('Failed to figure out extracted_dir: %s' % output)

                        if not sha256:
                            sha256 = get_sha256(s_path)
                        f.write('%s *%s\n' % (sha256, s))

                suffix = get_sha256(outfile)[:7]
                tarball = '%s_%s.tar.gz' % (i, suffix)
                cmd = 'tar --numeric-owner --remove-files %s %s %s' % (tar_opt, tarball, i)
                subprocess.check_call(cmd, shell=True, cwd=dst_dir)
                sf.write('%s\n' % tarball)
            sf.close()

    def do_upload(self):
        logger.info('Preparing upload files...')
        self.local_upload_dir = os.path.join(self.outdir, 'upload')
        date = time.strftime("%Y-%m-%d")
        self.local_upload_dir_date = os.path.join(self.local_upload_dir, date)
        if os.path.exists(self.local_upload_dir):
            self.mv_to_old('../upload')

        os.makedirs(self.local_upload_dir)

        local_handoff_dir = os.path.join(self.local_upload_dir_date, self.handoff_name)
        if not os.path.exists(local_handoff_dir):
            os.makedirs(local_handoff_dir)

        dst_dir_container = os.path.join(self.local_upload_dir_date, 'containers')
        dst_dir_ws = os.path.join(self.local_upload_dir_date, 'ws')
        os.makedirs(dst_dir_container)
        os.makedirs(dst_dir_ws)
        local_sources = os.path.join(local_handoff_dir, 'sources')
        os.makedirs(local_sources)
        for m in os.listdir(self.handoff_dir):
            if m == 'prserv':
                continue
            file_prefix = os.path.join(self.handoff_dir, m)
            if os.path.exists(file_prefix):
                if m.startswith('sources-'):
                    cmd = 'cp -aln %s/* %s' % (file_prefix, local_sources)
                    run_cmd(cmd)
                    continue

                if m.startswith('%s-' % self.handoff_name):
                    cmd = 'cp -alf %s %s' % (file_prefix, dst_dir_ws)
                    run_cmd(cmd)
                    continue

                dst_dir_tgt = os.path.join(local_handoff_dir, m)
                if not os.path.exists(dst_dir_tgt):
                    os.makedirs(dst_dir_tgt)
                for d in os.listdir(file_prefix):
                    d_path = os.path.join(file_prefix, d)
                    cmd = ""
                    if d.startswith('container-'):
                        cmd = 'cp -alf %s %s' % (d_path, dst_dir_container)
                    elif d.endswith('.tar.bz2'):
                        cmd = 'cp -alf %s %s' % (d_path, dst_dir_tgt)
                    elif d == 'repos':
                        repos_path = os.path.join(dst_dir_tgt, d)
                        if not os.path.exists(repos_path):
                            os.makedirs(repos_path)
                        cmd = 'cp -alf %s/* %s' % (d_path, repos_path)

                    if cmd:
                        run_cmd(cmd)

        if os.path.exists(self.pr_dir):
            shutil.copytree(self.pr_dir, os.path.join(local_handoff_dir, 'prserv'))

        # Create a symlink to latest dir
        os.symlink(date, os.path.join(self.local_upload_dir, 'latest'))

        logger.info('Done. Files are prepared in %s' % self.local_upload_dir)

        # Run rsync to remote url. Note, the slash '/' in cmd is very
        # important, otherwise, rsync would sync upload itself, but not the
        # files inside it.
        cmd = 'rsync -a %s/ %s' % (self.local_upload_dir, self.args.upload_url)
        logger.info('Running %s' % cmd)
        subprocess.check_call(cmd, shell=True)

    def is_ci_branch(self):
        try:
            g = git.cmd.Git(self.data_dir)
            head_name = g.rev_parse('--symbolic-full-name', 'm/master')
            if head_name.endswith('/WRLINUX_CI'):
                return True
        except Exception as e:
            logger.error('Failed to check ci branch: %s' % e)

        return False

    def download_rpm(self):
        logger.info('Downloading rpm packages ...')
        if not os.path.exists(self.deploy_dir_rpm):
            os.makedirs(self.deploy_dir_rpm)
        cmd = 'rsync -a --delete %s/%s/repos/rpm/ %s' % (self.args.rpm_url, self.machine, self.deploy_dir_rpm)
        run_cmd(cmd)

    def gen_images(self):
        self.gen_ostree()
        self.gen_container()
        self.gen_sdk()
        self.gen_feeds()
        self.post()

def main():
    gen = GenImage()

    if gen.args.upload_url:
        gen.do_upload()
    elif gen.args.config:
        gen.read_config()
    else:
        gen.prepare()
        # Generate both images and sources
        if gen.args.ip_review and gen.args.build_images:
            gen.gen_images()
            gen.ip_review()
        # Generate sources only
        elif gen.args.ip_review:
            gen.ip_review()
        # Generate binary images by default
        else:
            gen.gen_images()

if __name__ == "__main__":
    try:
        ret = main()
    except Exception as esc:
        ret = 1
        import traceback
        traceback.print_exc()
    sys.exit(ret)
